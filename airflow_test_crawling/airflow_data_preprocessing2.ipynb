{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Using cached boto3-1.34.145-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: botocore in c:\\users\\dldud\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.34.29)\n",
      "Collecting s3transfer\n",
      "  Using cached s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting botocore\n",
      "  Using cached botocore-1.34.145-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\dldud\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\dldud\\appdata\\roaming\\python\\python311\\site-packages (from botocore) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\dldud\\appdata\\roaming\\python\\python311\\site-packages (from botocore) (1.26.18)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dldud\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.16.0)\n",
      "Using cached boto3-1.34.145-py3-none-any.whl (139 kB)\n",
      "Using cached botocore-1.34.145-py3-none-any.whl (12.4 MB)\n",
      "Using cached s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "Installing collected packages: botocore, s3transfer, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.34.29\n",
      "    Uninstalling botocore-1.34.29:\n",
      "      Successfully uninstalled botocore-1.34.29\n",
      "Successfully installed boto3-1.34.145 botocore-1.34.145 s3transfer-0.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awsebcli 3.20.10 requires botocore<1.32.0,>1.23.41, but you have botocore 1.34.145 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install boto3 botocore s3transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "\n",
    "# AWS S3 Î≤ÑÌÇ∑ Ï†ïÎ≥¥\n",
    "bucket_name = 'papalio-test-bucket'\n",
    "file_key = 'test_otto/products_with_size_color.csv'\n",
    "\n",
    "# S3 ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÉùÏÑ±\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3ÏóêÏÑú CSV ÌååÏùº Îã§Ïö¥Î°úÎìú\n",
    "try:\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    \n",
    "    # CSV ÎÇ¥Ïö©ÏùÑ Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏúºÎ°ú Î≥ÄÌôò\n",
    "    df = pd.read_csv(io.StringIO(content))\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ Ï∂úÎ†•\n",
    "    #print(df)\n",
    "    #df csvÌååÏùºÎ°ú ÎßåÎì§Í∏∞\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading S3 object: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "\n",
    "'''\n",
    "Ïù¥ Îã®ÏùÄ product tableÏùÑ Ï†ÑÏ≤òÎ¶¨ÌïòÎäî Î∂ÄÎ∂ÑÏûÑ\n",
    "'''\n",
    "\n",
    "#production preprocessing\n",
    "df['product_id'] = df.index.to_series().apply(lambda x: f'top_{x}')\n",
    "# price Ïó¥Ïùò Îç∞Ïù¥ÌÑ∞Î•º Ï†ÑÏ≤òÎ¶¨\n",
    "for i in range(len(df)):\n",
    "    price_value = df.loc[i, 'price']\n",
    "    # ÎπàÏπ∏ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú ÏïûÎ∂ÄÎ∂ÑÎßå ÎÇ®Í∏∞Í≥† ÎÇòÎ®∏ÏßÄ Ï†úÍ±∞\n",
    "    price_value = price_value.split(' ')[0]\n",
    "    # Ïà´ÏûêÏôÄ ÏâºÌëúÎßå ÎÇ®Í∏∞Í∏∞\n",
    "    price_value = re.sub(r'[^\\d,]', '', price_value)\n",
    "    df.loc[i, 'price'] = price_value\n",
    "\n",
    "\n",
    "#color_options ÎπÑÏö∞Í∏∞\n",
    "\n",
    "#categoryÎ•º Ï†ÑÎ∂Ä topÏúºÎ°ú Î∞îÍæ∏Í∏∞\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"preprocessing_test.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 549 (char 548)\n",
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 1827 (char 1826)\n",
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 436 (char 435)\n",
      "Error decoding JSON: Invalid \\escape: line 1 column 1440 (char 1439)\n",
      "Saved all reviews to musinsa_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# CSV ÌååÏùº ÏùΩÍ∏∞\n",
    "df = pd.read_csv('preprocessing_test.csv')\n",
    "\n",
    "# reviews Ïª¨ÎüºÏùò JSON Îç∞Ïù¥ÌÑ∞Î•º ÌååÏã±ÌïòÎäî Ìï®Ïàò\n",
    "def parse_reviews(review_str):\n",
    "    try:\n",
    "        return json.loads(review_str.replace(\"'\", '\"'))\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "# Î™®Îì† Î¶¨Î∑∞Î•º Îã¥ÏùÑ Î¶¨Ïä§Ìä∏\n",
    "all_reviews = []\n",
    "\n",
    "# Í∞Å product_nameÎ≥ÑÎ°ú Î¶¨Î∑∞Î•º Ï∂îÏ∂úÌïòÏó¨ ÌÜµÌï©\n",
    "for index, row in df.iterrows():\n",
    "    product_name = row['product_name']\n",
    "    reviews_list = parse_reviews(row['reviews'])\n",
    "    \n",
    "    for review in reviews_list:\n",
    "        review['product_name'] = product_name  # Í∞Å Î¶¨Î∑∞Ïóê Ï†úÌíà Ïù¥Î¶Ñ Ï∂îÍ∞Ä\n",
    "        all_reviews.append(review)\n",
    "        \n",
    "# ÌÜµÌï© Î¶¨Î∑∞ Îç∞Ïù¥ÌÑ∞Î•º Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏúºÎ°ú Î≥ÄÌôò\n",
    "reviews_df = pd.DataFrame(all_reviews)\n",
    "\n",
    "# weight, height, genderÎ•º Î∂ÑÎ¶¨ÌïòÏó¨ ÏÉàÎ°úÏö¥ Ïª¨Îüº ÏÉùÏÑ±\n",
    "def split_weight_height_gender(whg):\n",
    "    parts = whg.split(' ¬∑ ')\n",
    "    if len(parts) == 3:\n",
    "        gender, height, weight = parts\n",
    "        height = height.replace('cm', '')\n",
    "        weight = weight.replace('kg', '')\n",
    "        return gender.strip(), height.strip(), weight.strip()\n",
    "    return None, None, None\n",
    "\n",
    "reviews_df['gender'], reviews_df['height'], reviews_df['weight'] = zip(*reviews_df['weight_height_gender'].map(split_weight_height_gender))\n",
    "\n",
    "# Ïª¨ÎüºÎ™Ö Î≥ÄÍ≤Ω\n",
    "reviews_df.rename(columns={'top_size': 'size_option', 'purchased_size': 'size'}, inplace=True)\n",
    "\n",
    "# ÏÉàÎ°úÏö¥ Ïª¨Îüº top_sizeÏôÄ bottom_size ÏÉùÏÑ±\n",
    "reviews_df['top_size'] = 'none'\n",
    "reviews_df['bottom_size'] = 'none'\n",
    "# Ïª¨Îüº ÏÇ≠Ï†ú\n",
    "reviews_df.drop(columns=['weight_height_gender'], inplace=True)\n",
    "\n",
    "# ÌÜµÌï© Î¶¨Î∑∞ Îç∞Ïù¥ÌÑ∞Î•º CSV ÌååÏùºÎ°ú Ï†ÄÏû•\n",
    "reviews_df.to_csv('musinsa_reviews.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"Saved all reviews to musinsa_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 549 (char 548)\n",
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 1827 (char 1826)\n",
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 436 (char 435)\n",
      "Error decoding JSON: Invalid \\escape: line 1 column 1440 (char 1439)\n",
      "Saved all reviews to musinsa_reviews.csv\n",
      "Saved processed products to processed_products.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Ïù¥ ÏΩîÎìúÎäî ÎÇ¥ Ï†ÑÏ≤òÎ¶¨ ÌååÌä∏ÏûÑ.\n",
    "'''\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "\n",
    "# AWS S3 Î≤ÑÌÇ∑ Ï†ïÎ≥¥\n",
    "bucket_name = 'papalio-test-bucket'\n",
    "file_key = 'test_otto/products_with_size_color.csv'\n",
    "\n",
    "# S3 ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÉùÏÑ±\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3ÏóêÏÑú CSV ÌååÏùº Îã§Ïö¥Î°úÎìú\n",
    "try:\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    \n",
    "    # CSV ÎÇ¥Ïö©ÏùÑ Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏúºÎ°ú Î≥ÄÌôò\n",
    "    df = pd.read_csv(io.StringIO(content))\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ Ï∂úÎ†•\n",
    "    #print(df)\n",
    "    #df csvÌååÏùºÎ°ú ÎßåÎì§Í∏∞\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading S3 object: {e}\")\n",
    "\n",
    "# Production preprocessing\n",
    "df['product_id'] = 'none'\n",
    "df.insert(0, 'rank', [i for i in range(0, len(df))])\n",
    "\n",
    "# price Ïó¥Ïùò Îç∞Ïù¥ÌÑ∞Î•º Ï†ÑÏ≤òÎ¶¨\n",
    "for i in range(len(df)):\n",
    "    price_value = df.loc[i, 'price']\n",
    "    # ÎπàÏπ∏ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú ÏïûÎ∂ÄÎ∂ÑÎßå ÎÇ®Í∏∞Í≥† ÎÇòÎ®∏ÏßÄ Ï†úÍ±∞\n",
    "    price_value = price_value.split(' ')[0]\n",
    "    # Ïà´ÏûêÏôÄ ÏâºÌëúÎßå ÎÇ®Í∏∞Í∏∞\n",
    "    price_value = re.sub(r'[^\\d,]', '', price_value)\n",
    "    df.loc[i, 'price'] = price_value\n",
    "\n",
    "# color_options ÎπÑÏö∞Í∏∞\n",
    "df['color_options'] = 'none'\n",
    "\n",
    "# categoryÎ•º Ï†ÑÎ∂Ä 'top'ÏúºÎ°ú Î∞îÍæ∏Í∏∞\n",
    "df['category'] = 'top'\n",
    "df.drop(columns=['size_options'],inplace=True)\n",
    "\n",
    "\n",
    "# Î¶¨Î∑∞ Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨\n",
    "# reviews Ïª¨ÎüºÏùò JSON Îç∞Ïù¥ÌÑ∞Î•º ÌååÏã±ÌïòÎäî Ìï®Ïàò\n",
    "def parse_reviews(review_str):\n",
    "    try:\n",
    "        return json.loads(review_str.replace(\"'\", '\"'))\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "# Î™®Îì† Î¶¨Î∑∞Î•º Îã¥ÏùÑ Î¶¨Ïä§Ìä∏\n",
    "all_reviews = []\n",
    "\n",
    "# Í∞Å product_nameÎ≥ÑÎ°ú Î¶¨Î∑∞Î•º Ï∂îÏ∂úÌïòÏó¨ ÌÜµÌï©\n",
    "for index, row in df.iterrows():\n",
    "    product_name = row['product_name']\n",
    "    reviews_list = parse_reviews(row['reviews'])\n",
    "    \n",
    "    for review in reviews_list:\n",
    "        review['product_name'] = product_name  # Í∞Å Î¶¨Î∑∞Ïóê Ï†úÌíà Ïù¥Î¶Ñ Ï∂îÍ∞Ä\n",
    "        all_reviews.append(review)\n",
    "        \n",
    "# ÌÜµÌï© Î¶¨Î∑∞ Îç∞Ïù¥ÌÑ∞Î•º Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏúºÎ°ú Î≥ÄÌôò\n",
    "reviews_df = pd.DataFrame(all_reviews)\n",
    "\n",
    "# weight, height, genderÎ•º Î∂ÑÎ¶¨ÌïòÏó¨ ÏÉàÎ°úÏö¥ Ïª¨Îüº ÏÉùÏÑ±\n",
    "def split_weight_height_gender(whg):\n",
    "    parts = whg.split(' ¬∑ ')\n",
    "    if len(parts) == 3:\n",
    "        gender, height, weight = parts\n",
    "        height = height.replace('cm', '')\n",
    "        weight = weight.replace('kg', '')\n",
    "        return gender.strip(), height.strip(), weight.strip()\n",
    "    return None, None, None\n",
    "\n",
    "reviews_df['gender'], reviews_df['height'], reviews_df['weight'] = zip(*reviews_df['weight_height_gender'].map(split_weight_height_gender))\n",
    "\n",
    "# weight_height_gender Ïª¨Îüº ÏÇ≠Ï†ú\n",
    "reviews_df.drop(columns=['weight_height_gender'], inplace=True)\n",
    "\n",
    "# Ïª¨ÎüºÎ™Ö Î≥ÄÍ≤Ω\n",
    "reviews_df.rename(columns={'top_size': 'size_option', 'purchased_size': 'size'}, inplace=True)\n",
    "reviews_df.drop(columns=['purchased_product_id'],inplace=True)\n",
    "\n",
    "# ÏÉàÎ°úÏö¥ Ïª¨Îüº top_sizeÏôÄ bottom_size ÏÉùÏÑ±\n",
    "reviews_df['quality_comment'] = 'none'\n",
    "reviews_df['product_id'] = 'none'\n",
    "reviews_df['color'] = 'none'\n",
    "reviews_df['top_size'] = 'none'\n",
    "reviews_df['bottom_size'] = 'none'\n",
    "df.drop(columns=['reviews'],inplace=True)\n",
    "# ÌÜµÌï© Î¶¨Î∑∞ Îç∞Ïù¥ÌÑ∞Î•º CSV ÌååÏùºÎ°ú Ï†ÄÏû•\n",
    "reviews_df.to_csv('musinsa_reviews.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"Saved all reviews to musinsa_reviews.csv\")\n",
    "\n",
    "# Î≥ÄÍ≤ΩÎêú product ÌÖåÏù¥Î∏îÎèÑ CSV ÌååÏùºÎ°ú Ï†ÄÏû•\n",
    "df.to_csv('processed_products.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"Saved processed products to processed_products.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     review_id size_option brightness_comment color_comment thickness_comment  \\\n",
      "0    LV.4 Ïú†ÌïÑÎ¶¨Ìã∞          Ïª§Ïöî               Ïñ¥ÎëêÏõåÏöî           ÌùêÎ†§Ïöî              ÎëêÍ∫ºÏõåÏöî   \n",
      "1  LV.5 Ïö©ÎëêÏÇ∞Ìò∏Ïπ¥Í≤å       Î≥¥ÌÜµÏù¥ÏóêÏöî              Î≥¥ÌÜµÏù¥ÏóêÏöî         Î≥¥ÌÜµÏù¥ÏóêÏöî             Î≥¥ÌÜµÏù¥ÏóêÏöî   \n",
      "2  LV.7 ÌôÄÎ¶¨Î™∞Î¶¨Î°§Î¶¨       Î≥¥ÌÜµÏù¥ÏóêÏöî              Î≥¥ÌÜµÏù¥ÏóêÏöî         Î≥¥ÌÜµÏù¥ÏóêÏöî             Î≥¥ÌÜµÏù¥ÏóêÏöî   \n",
      "3   LV.6 ÎπÑÏ¢ÖÌîÑÎ¶¨Ï†ú       Î≥¥ÌÜµÏù¥ÏóêÏöî              Î≥¥ÌÜµÏù¥ÏóêÏöî         Î≥¥ÌÜµÏù¥ÏóêÏöî             Î≥¥ÌÜµÏù¥ÏóêÏöî   \n",
      "4     LV.8 ÎπôÌïòÏôï       Î≥¥ÌÜµÏù¥ÏóêÏöî              Î≥¥ÌÜµÏù¥ÏóêÏöî         Î≥¥ÌÜµÏù¥ÏóêÏöî             Î≥¥ÌÜµÏù¥ÏóêÏöî   \n",
      "\n",
      "  size                               comment            product_name gender  \\\n",
      "0   XL        ÏûëÎÖÑÏóê ÏÇøÎçòÍ±∞ ÏûòÏûÖÍ≥†ÏûàÏñ¥ÏÑú Í∞ôÏùÄÏÉâ Í∞ôÏùÄÍ±∏Î°ú Îñ†Ï£ºÎ¨∏ÌñáÏñ¥Ïöî  Ìó§ÎπÑÏõ®Ïù¥Ìä∏ Ïò§Î≤ÑÏÇ¨Ïù¥Ï¶à Ïä§Ïõ®Ìä∏ÏÖîÏ∏† [Î∏îÎûô]     ÎÇ®ÏÑ±   \n",
      "1    L              ÎëêÍªçÍ∏∞Î≥¥Îã® Îî¥Îî¥Ìï¥Ïöî ÎßàÏπò Ï†ú Ïã§Ï†ÑÏïïÏ∂ïÍ∑ºÏú°Ï≤òÎüº  Ìó§ÎπÑÏõ®Ïù¥Ìä∏ Ïò§Î≤ÑÏÇ¨Ïù¥Ï¶à Ïä§Ïõ®Ìä∏ÏÖîÏ∏† [Î∏îÎûô]     ÎÇ®ÏÑ±   \n",
      "2    L  Ïù¥ÏõêÎã®Ïóê 2ÎßåÏõê? ÏßÑÏßú ÎßêÎèÑÏïàÎêòÎäî Í∞ÄÏÑ±ÎπÑÌÖúÏûÖÎãàÎã§ ÏÇ¨ÎûëÌï¥Ïöî Î¨¥ÌÉ†Îã§Îìú  Ìó§ÎπÑÏõ®Ïù¥Ìä∏ Ïò§Î≤ÑÏÇ¨Ïù¥Ï¶à Ïä§Ïõ®Ìä∏ÏÖîÏ∏† [Î∏îÎûô]     ÎÇ®ÏÑ±   \n",
      "3  2XL   Ï¢ãÏïÑÏöî ÎëêÍπ®Í∞êÎèÑ ÏûàÏñ¥ÏÑú ÌÉÑÌÉÑÌï¥ÏÑú Ïò§Î≤ÑÌïòÍ≤å\\nÏûÖÏùÑÎùºÍ≥† ÌÅ¨Í≤å ÏÇøÏñ¥Ïöî  Ìó§ÎπÑÏõ®Ïù¥Ìä∏ Ïò§Î≤ÑÏÇ¨Ïù¥Ï¶à Ïä§Ïõ®Ìä∏ÏÖîÏ∏† [Î∏îÎûô]     ÎÇ®ÏÑ±   \n",
      "4    L         ÎßàÏùåÏóê Îì§Ïñ¥ÏÑú Îã§Î•∏ ÏÉâÎèÑ Íµ¨Îß§Ìï©ÎãàÎã§ Ìó§ÎπÑÏõ®Ïù¥Ìä∏ ÎßõÏßë!  Ìó§ÎπÑÏõ®Ïù¥Ìä∏ Ïò§Î≤ÑÏÇ¨Ïù¥Ï¶à Ïä§Ïõ®Ìä∏ÏÖîÏ∏† [Î∏îÎûô]     ÎÇ®ÏÑ±   \n",
      "\n",
      "   height  weight top_size bottom_size  \n",
      "0   176.0    85.0     none        none  \n",
      "1   176.0    63.0     none        none  \n",
      "2   176.0    56.0     none        none  \n",
      "3   184.0    82.0     none        none  \n",
      "4   168.0    67.0     none        none  \n",
      "   rank product_id                product_name category   price  \\\n",
      "0     0       none      Ìó§ÎπÑÏõ®Ïù¥Ìä∏ Ïò§Î≤ÑÏÇ¨Ïù¥Ï¶à Ïä§Ïõ®Ìä∏ÏÖîÏ∏† [Î∏îÎûô]      top  34,454   \n",
      "1     1       none        24S/S Ïò§Î≤ÑÌïè ÌîºÏºÄÌã∞ÏÖîÏ∏† (Î∏îÎûô)      top  34,738   \n",
      "2     2       none           ÌôîÎûÄ ÏÑ∏ÎØ∏Ïò§Î≤Ñ ÎãàÌä∏ Î™®Ïπ¥ Î≤†Ïù¥ÏßÄ      top  66,076   \n",
      "3     3       none  Ïò§Î≤ÑÏÇ¨Ïù¥Ï¶à ÏßëÏóÖ Ïπ¥Îùº ÌÑ∞ÌãÄÎÑ• ÎãàÌä∏ [BLACK]      top  52,600   \n",
      "4     4       none      460G Ïª∑ Ìó§ÎπÑ ÌîºÍ∑∏Î®ºÌä∏ Ìã∞ÏÖîÏ∏†-Ï∞®ÏΩú-      top  36,600   \n",
      "\n",
      "                                           image_url  \\\n",
      "0  https://image.msscdn.net/images/goods_img/2020...   \n",
      "1  https://image.msscdn.net/images/goods_img/2019...   \n",
      "2  https://image.msscdn.net/images/goods_img/2019...   \n",
      "3  https://image.msscdn.net/images/goods_img/2021...   \n",
      "4  https://image.msscdn.net/images/goods_img/2021...   \n",
      "\n",
      "                                 description  \\\n",
      "0  https://www.musinsa.com/app/goods/1642887   \n",
      "1  https://www.musinsa.com/app/goods/1031260   \n",
      "2  https://www.musinsa.com/app/goods/1139099   \n",
      "3  https://www.musinsa.com/app/goods/2226766   \n",
      "4  https://www.musinsa.com/app/goods/1944612   \n",
      "\n",
      "                                         size color_options  \n",
      "0         ['S', 'M', 'L', 'XL', '2XL', '3XL']          none  \n",
      "1                ['S', 'M', 'L', 'XL', 'XXL']          none  \n",
      "2  ['42 (FOR WOMAN)', '44', '46', '48', '50']          none  \n",
      "3                      ['WS', 'M', 'L', 'XL']          none  \n",
      "4                       ['S', 'M', 'L', 'XL']          none  \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Ïù¥ ÏΩîÎìúÎäî ÎÇ¥ Ï†ÑÏ≤òÎ¶¨ ÌååÌä∏ÏûÑ.\n",
    "'''\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "\n",
    "# AWS S3 Î≤ÑÌÇ∑ Ï†ïÎ≥¥\n",
    "bucket_name = 'papalio-test-bucket'\n",
    "file_key2 = 'test_otto/musinsa_reviews.csv'\n",
    "file_key3 = 'test_otto/processed_products.csv'\n",
    "\n",
    "# S3 ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÉùÏÑ±\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3ÏóêÏÑú CSV ÌååÏùº Îã§Ïö¥Î°úÎìú\n",
    "try:\n",
    "    # Ï≤´ Î≤àÏß∏ ÌååÏùº Îã§Ïö¥Î°úÎìú Î∞è DataFrameÏúºÎ°ú Î≥ÄÌôò\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=file_key2)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    df = pd.read_csv(io.StringIO(content))\n",
    "    \n",
    "    # Îëê Î≤àÏß∏ ÌååÏùº Îã§Ïö¥Î°úÎìú Î∞è DataFrameÏúºÎ°ú Î≥ÄÌôò\n",
    "    response2 = s3.get_object(Bucket=bucket_name, Key=file_key3)\n",
    "    content2 = response2['Body'].read().decode('utf-8')\n",
    "    df2 = pd.read_csv(io.StringIO(content2))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reading S3 object: {e}\")\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Ï∂úÎ†•\n",
    "print(df.head())\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder non-integrated-data created in bucket otto-glue\n",
      "Uploaded 29cm_products.csv to otto-glue/non-integrated-data/29cm_products.csv\n",
      "Uploaded 29cm_reviews.csv to otto-glue/non-integrated-data/29cm_reviews.csv\n",
      "Uploaded processed_products.csv to otto-glue/non-integrated-data/processed_products.csv\n",
      "Uploaded musinsa_reviews.csv to otto-glue/non-integrated-data/musinsa_reviews.csv\n",
      "Uploaded zigzag_product_infos.csv to otto-glue/non-integrated-data/zigzag_product_infos.csv\n",
      "Uploaded zigzag_reviews.csv to otto-glue/non-integrated-data/zigzag_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# AWS ÏûêÍ≤© Ï¶ùÎ™Ö ÏÑ§Ï†ï\n",
    "aws_access_key_id = 'AKIA4RRVVY55QSQPIXPQ'\n",
    "aws_secret_access_key = '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'\n",
    "\n",
    "# S3 ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÉùÏÑ±\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "\n",
    "bucket_name = 'otto-glue'\n",
    "folder_name = 'non-integrated-data'\n",
    "local_files = [\n",
    "    '29cm_products.csv',\n",
    "    '29cm_reviews.csv',\n",
    "    'processed_products.csv',\n",
    "    'musinsa_reviews.csv',\n",
    "    'zigzag_product_infos.csv',\n",
    "    'zigzag_reviews.csv'\n",
    "]\n",
    "\n",
    "def upload_files_to_s3(bucket_name, folder_name, local_files):\n",
    "    for file_name in local_files:\n",
    "        try:\n",
    "            s3_client.upload_file(file_name, bucket_name, f\"{folder_name}/{file_name}\")\n",
    "            print(f\"Uploaded {file_name} to {bucket_name}/{folder_name}/{file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {file_name}: {e}\")\n",
    "\n",
    "# Ìè¥Îçî ÎßåÎì§Í∏∞\n",
    "def create_s3_folder(bucket_name, folder_name):\n",
    "    try:\n",
    "        s3_client.put_object(Bucket=bucket_name, Key=(folder_name+'/'))\n",
    "        print(f\"Folder {folder_name} created in bucket {bucket_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating folder {folder_name}: {e}\")\n",
    "\n",
    "# Ïã§Ìñâ\n",
    "create_s3_folder(bucket_name, folder_name)\n",
    "upload_files_to_s3(bucket_name, folder_name, local_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Reviews DataFrame Columns:\n",
      "Index(['review_id', 'product_name', 'color', 'size', 'height', 'gender',\n",
      "       'weight', 'top_size', 'bottom_size', 'size_comment', 'quality_comment',\n",
      "       'color_comment', 'thickness_comment', 'brightness_comment', 'comment',\n",
      "       'product_name112538672_1', '112538672_1', '112538672', '(Î°±Î°±ver)Î∏îÎûô',\n",
      "       'one size', 'Ï†ïÏÇ¨Ïù¥Ï¶àÏòàÏöî', 'ÏïÑÏ£º ÎßåÏ°±Ìï¥Ïöî', 'ÌôîÎ©¥Í≥º ÎπÑÏä∑Ìï¥Ïöî', '156cm', '50kg',\n",
      "       'Ïù¥Í∞ÄÍ≤©Ïóê, Ïù¥ ÏòµÏÖò ÏàòÏóê, Ïù¥Îü∞ Ï¥ùÏïåÎ∞∞ÏÜ°??? ÎßåÏ°±ÎèÑ ÏµúÏÉÅ. ÎãπÏû• ÏÇ¨Ïã≠ÏãúÏò§. Ï†ÄÎäî 157ÏÑºÏπò Ïä§ÌéôÏóê, ÌôîÏù¥Ìä∏ Î°± ver. Íµ¨Îß§ÌñàÏñ¥Ïöî (Í∑ºÎç∞ ÎÑàÎ¨¥ Î≤åÏç® ÎΩïÏùÑ ÎΩëÏïÑÏÑú Î∏îÎûô Î°±Î°± ver. Ï∂îÍ∞Ä Íµ¨Îß§Ìï®) Ìå®Îìú ÏïàÎπÑÏ≥êÏöî üí• Ìå®Îìú ÏïàÎπÑÏπ®, Ïã§Ï°¥. \\nÏ∫°Î™®Ïñë Î°úÏºìÎ∞ú‚Ä¶\\nÎçîÎ≥¥Í∏∞',\n",
      "       'ÏÉÅÏùò 55', 'none', '[Î¨¥Î£åÎ∞∞ÏÜ°][3ÎßåÏû•ÌåêÎß§] Ìã∞Î≤Ñ ÏôÄÏù¥Îìú ÏΩîÌäº ÏÖîÏ∏†'],\n",
      "      dtype='object')\n",
      "Combined Products DataFrame Columns:\n",
      "Index(['product_id', 'rank', 'product_name', 'category', 'price', 'image_url',\n",
      "       'description', 'color', 'size', 'platform'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import random\n",
    "\n",
    "# AWS ÏûêÍ≤© Ï¶ùÎ™Ö ÏÑ§Ï†ï\n",
    "aws_access_key_id = 'A-PQ'\n",
    "aws_secret_access_key = '-'\n",
    "\n",
    "# S3 ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÉùÏÑ±\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "\n",
    "color_list = [\"red\", \"blue\", \"green\", \"yellow\", \"black\", \"white\", \"gray\", \"orange\", \"purple\", \"brown\",\n",
    "              \"pink\", \"cyan\", \"magenta\", \"lime\", \"teal\", \"lavender\", \"beige\", \"maroon\", \"olive\", \"navy\"]\n",
    "\n",
    "bucket_name = 'otto-glue'\n",
    "folder_name = 'non-integrated-data'\n",
    "\n",
    "product_file_names = [\n",
    "    '29cm_products.csv',\n",
    "    'processed_products.csv',\n",
    "    'zigzag_products.csv',\n",
    "]\n",
    "review_file_names = [\n",
    "    '29cm_reviews.csv',\n",
    "    'musinsa_reviews.csv',\n",
    "    'zigzag_reviews.csv'\n",
    "]\n",
    "\n",
    "def read_s3_csv(bucket_name, file_key):\n",
    "    obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    df = pd.read_csv(obj['Body'])\n",
    "    return df\n",
    "\n",
    "def process_reviews(df, is_zigzag=False):\n",
    "    if is_zigzag:\n",
    "        df.drop(columns=['Unnamed: 0', 'product_id'], inplace=True, errors='ignore')\n",
    "        df['brightness_comment'] = 'none'\n",
    "        df['gender'] = 'none'\n",
    "        df['thickness_comment'] = 'none'\n",
    "    return df\n",
    "\n",
    "def process_products(df, platform_name, is_zigzag=False):\n",
    "    if is_zigzag:\n",
    "        df.drop(columns=['Unnamed: 0'], inplace=True, errors='ignore')\n",
    "    df['platform'] = platform_name\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # Î¶¨Î∑∞ ÌååÏùº ÌÜµÌï©\n",
    "    review_dfs = []\n",
    "    for file_name in review_file_names:\n",
    "        file_key = f'{folder_name}/{file_name}'\n",
    "        df = read_s3_csv(bucket_name, file_key)\n",
    "        if 'zigzag' in file_name:\n",
    "            df = process_reviews(df, is_zigzag=True)\n",
    "        review_dfs.append(df)\n",
    "\n",
    "    combined_reviews_df = pd.concat(review_dfs).drop_duplicates().reset_index(drop=True)\n",
    "    print(\"Combined Reviews DataFrame Columns:\")\n",
    "    print(combined_reviews_df.columns)\n",
    "\n",
    "    # Ï†úÌíà ÌååÏùº ÌÜµÌï©\n",
    "    product_dfs = []\n",
    "    platform_mapping = {\n",
    "        '29cm_products.csv': '29cm',\n",
    "        'processed_products.csv': 'musinsa',\n",
    "        'zigzag_products.csv': 'zigzag'\n",
    "    }\n",
    "\n",
    "    for file_name in product_file_names:\n",
    "        file_key = f'{folder_name}/{file_name}'\n",
    "        df = read_s3_csv(bucket_name, file_key)\n",
    "        platform_name = platform_mapping[file_name]\n",
    "        if 'zigzag' in file_name:\n",
    "            df = process_products(df, platform_name, is_zigzag=True)\n",
    "        else:\n",
    "            df = process_products(df, platform_name)\n",
    "        product_dfs.append(df)\n",
    "\n",
    "    combined_products_df = pd.concat(product_dfs).drop_duplicates().reset_index(drop=True)\n",
    "    print(\"Combined Products DataFrame Columns:\")\n",
    "    print(combined_products_df.columns)\n",
    "\n",
    "    # ÏÉâÏÉÅ Ï∂îÍ∞Ä\n",
    "    def add_random_colors(row):\n",
    "        if row['color'] == 'none':\n",
    "            num_colors = random.randint(1, 4)\n",
    "            row['color'] = ', '.join(random.sample(color_list, num_colors))\n",
    "        return row\n",
    "\n",
    "    combined_products_df = combined_products_df.apply(add_random_colors, axis=1)\n",
    "\n",
    "    # CSV ÌååÏùº Ï†ÄÏû•\n",
    "    combined_reviews_df.to_csv('combined_reviews.csv', index=False, encoding='utf-8-sig')\n",
    "    combined_products_df.to_csv('combined_products.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchKey",
     "evalue": "An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchKey\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 50\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m#test_df.to_csv('test_zigzag_reviews.csv', index=False, encoding='utf-8-sig')\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m#test_df2.to_csv('test_zigzag_product.csv', index=False, encoding='utf-8-sig' )\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 50\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# combined_reviews.csv Î∂àÎü¨Ïò§Í∏∞\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     combined_reviews_df \u001b[38;5;241m=\u001b[39m \u001b[43mread_s3_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_reviews_file_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCombined Reviews DataFrame Columns:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(combined_reviews_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist())\n",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m, in \u001b[0;36mread_s3_csv\u001b[1;34m(bucket_name, file_key)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_s3_csv\u001b[39m(bucket_name, file_key):\n\u001b[1;32m---> 25\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43ms3_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\dldud\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\botocore\\client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m     )\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dldud\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\botocore\\client.py:1021\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m   1017\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1019\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[1;31mNoSuchKey\u001b[0m: An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist."
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# AWS ÏûêÍ≤© Ï¶ùÎ™Ö ÏÑ§Ï†ï\n",
    "aws_access_key_id = 'AKIA4RRVVY55QSQPIXPQ'\n",
    "aws_secret_access_key = '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'\n",
    "\n",
    "# S3 ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÉùÏÑ±\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "\n",
    "bucket_name = 'otto-glue'\n",
    "folder_name = 'integrated-data'\n",
    "\n",
    "combined_reviews_file_key = 'integrated-data/combined_reviews_2024-07-29 08:38:46.040114.csv'\n",
    "combined_products_file_key = 'integrated-data/combined_products.csv'\n",
    "test_key = 'non-integrated-data/zigzag_reviews.csv'\n",
    "test_key2 = 'non-integrated-data/zigzag_products.csv'\n",
    "\n",
    "def read_s3_csv(bucket_name, file_key):\n",
    "    obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    df = pd.read_csv(obj['Body'])\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # combined_reviews.csv Î∂àÎü¨Ïò§Í∏∞\n",
    "    combined_reviews_df = read_s3_csv(bucket_name, combined_reviews_file_key)\n",
    "    print(\"Combined Reviews DataFrame Columns:\")\n",
    "    print(combined_reviews_df.columns.tolist())\n",
    "\n",
    "    #combined_products.csv Î∂àÎü¨Ïò§Í∏∞\n",
    "    combined_products_df = read_s3_csv(bucket_name, combined_products_file_key)\n",
    "    print(\"Combined Products DataFrame Columns:\")\n",
    "    print(combined_products_df.columns.tolist())\n",
    "    \n",
    "    #test_df = read_s3_csv(bucket_name, test_key)\n",
    "    #test_df2 = read_s3_csv(bucket_name, test_key2)\n",
    "    #print(test_df.columns.tolist())\n",
    "    #print(test_df2.columns.tolist())\n",
    "    combined_reviews_df.to_csv('combined_reviews.csv', index=False, encoding='utf-8-sig')\n",
    "    combined_products_df.to_csv('combined_products.csv', index=False, encoding='utf-8-sig')\n",
    "    #test_df.to_csv('test_zigzag_reviews.csv', index=False, encoding='utf-8-sig')\n",
    "    #test_df2.to_csv('test_zigzag_product.csv', index=False, encoding='utf-8-sig' )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î¨∏ÏûêÏó¥ Î≥ÄÌôò :  2024-07-25 14:49:32\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "\n",
    "print(\"Î¨∏ÏûêÏó¥ Î≥ÄÌôò : \", now.strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
