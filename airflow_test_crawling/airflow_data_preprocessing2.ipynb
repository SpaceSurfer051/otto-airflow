{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Using cached boto3-1.34.145-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: botocore in c:\\users\\dldud\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.34.29)\n",
      "Collecting s3transfer\n",
      "  Using cached s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting botocore\n",
      "  Using cached botocore-1.34.145-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\dldud\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\dldud\\appdata\\roaming\\python\\python311\\site-packages (from botocore) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\dldud\\appdata\\roaming\\python\\python311\\site-packages (from botocore) (1.26.18)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dldud\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.16.0)\n",
      "Using cached boto3-1.34.145-py3-none-any.whl (139 kB)\n",
      "Using cached botocore-1.34.145-py3-none-any.whl (12.4 MB)\n",
      "Using cached s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "Installing collected packages: botocore, s3transfer, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.34.29\n",
      "    Uninstalling botocore-1.34.29:\n",
      "      Successfully uninstalled botocore-1.34.29\n",
      "Successfully installed boto3-1.34.145 botocore-1.34.145 s3transfer-0.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awsebcli 3.20.10 requires botocore<1.32.0,>1.23.41, but you have botocore 1.34.145 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install boto3 botocore s3transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "\n",
    "# AWS S3 버킷 정보\n",
    "bucket_name = 'papalio-test-bucket'\n",
    "file_key = 'test_otto/products_with_size_color.csv'\n",
    "\n",
    "# S3 클라이언트 생성\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3에서 CSV 파일 다운로드\n",
    "try:\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    \n",
    "    # CSV 내용을 데이터프레임으로 변환\n",
    "    df = pd.read_csv(io.StringIO(content))\n",
    "    \n",
    "    # 데이터프레임 출력\n",
    "    #print(df)\n",
    "    #df csv파일로 만들기\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading S3 object: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "\n",
    "'''\n",
    "이 단은 product table을 전처리하는 부분임\n",
    "'''\n",
    "\n",
    "#production preprocessing\n",
    "df['product_id'] = df.index.to_series().apply(lambda x: f'top_{x}')\n",
    "# price 열의 데이터를 전처리\n",
    "for i in range(len(df)):\n",
    "    price_value = df.loc[i, 'price']\n",
    "    # 빈칸을 기준으로 앞부분만 남기고 나머지 제거\n",
    "    price_value = price_value.split(' ')[0]\n",
    "    # 숫자와 쉼표만 남기기\n",
    "    price_value = re.sub(r'[^\\d,]', '', price_value)\n",
    "    df.loc[i, 'price'] = price_value\n",
    "\n",
    "\n",
    "#color_options 비우기\n",
    "\n",
    "#category를 전부 top으로 바꾸기\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"preprocessing_test.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 549 (char 548)\n",
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 1827 (char 1826)\n",
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 436 (char 435)\n",
      "Error decoding JSON: Invalid \\escape: line 1 column 1440 (char 1439)\n",
      "Saved all reviews to musinsa_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('preprocessing_test.csv')\n",
    "\n",
    "# reviews 컬럼의 JSON 데이터를 파싱하는 함수\n",
    "def parse_reviews(review_str):\n",
    "    try:\n",
    "        return json.loads(review_str.replace(\"'\", '\"'))\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "# 모든 리뷰를 담을 리스트\n",
    "all_reviews = []\n",
    "\n",
    "# 각 product_name별로 리뷰를 추출하여 통합\n",
    "for index, row in df.iterrows():\n",
    "    product_name = row['product_name']\n",
    "    reviews_list = parse_reviews(row['reviews'])\n",
    "    \n",
    "    for review in reviews_list:\n",
    "        review['product_name'] = product_name  # 각 리뷰에 제품 이름 추가\n",
    "        all_reviews.append(review)\n",
    "        \n",
    "# 통합 리뷰 데이터를 데이터프레임으로 변환\n",
    "reviews_df = pd.DataFrame(all_reviews)\n",
    "\n",
    "# weight, height, gender를 분리하여 새로운 컬럼 생성\n",
    "def split_weight_height_gender(whg):\n",
    "    parts = whg.split(' · ')\n",
    "    if len(parts) == 3:\n",
    "        gender, height, weight = parts\n",
    "        height = height.replace('cm', '')\n",
    "        weight = weight.replace('kg', '')\n",
    "        return gender.strip(), height.strip(), weight.strip()\n",
    "    return None, None, None\n",
    "\n",
    "reviews_df['gender'], reviews_df['height'], reviews_df['weight'] = zip(*reviews_df['weight_height_gender'].map(split_weight_height_gender))\n",
    "\n",
    "# 컬럼명 변경\n",
    "reviews_df.rename(columns={'top_size': 'size_option', 'purchased_size': 'size'}, inplace=True)\n",
    "\n",
    "# 새로운 컬럼 top_size와 bottom_size 생성\n",
    "reviews_df['top_size'] = 'none'\n",
    "reviews_df['bottom_size'] = 'none'\n",
    "# 컬럼 삭제\n",
    "reviews_df.drop(columns=['weight_height_gender'], inplace=True)\n",
    "\n",
    "# 통합 리뷰 데이터를 CSV 파일로 저장\n",
    "reviews_df.to_csv('musinsa_reviews.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"Saved all reviews to musinsa_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 549 (char 548)\n",
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 1827 (char 1826)\n",
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 436 (char 435)\n",
      "Error decoding JSON: Invalid \\escape: line 1 column 1440 (char 1439)\n",
      "Saved all reviews to musinsa_reviews.csv\n",
      "Saved processed products to processed_products.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "이 코드는 내 전처리 파트임.\n",
    "'''\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "\n",
    "# AWS S3 버킷 정보\n",
    "bucket_name = 'papalio-test-bucket'\n",
    "file_key = 'test_otto/products_with_size_color.csv'\n",
    "\n",
    "# S3 클라이언트 생성\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3에서 CSV 파일 다운로드\n",
    "try:\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    \n",
    "    # CSV 내용을 데이터프레임으로 변환\n",
    "    df = pd.read_csv(io.StringIO(content))\n",
    "    \n",
    "    # 데이터프레임 출력\n",
    "    #print(df)\n",
    "    #df csv파일로 만들기\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading S3 object: {e}\")\n",
    "\n",
    "# Production preprocessing\n",
    "df['product_id'] = 'none'\n",
    "df.insert(0, 'rank', [i for i in range(0, len(df))])\n",
    "\n",
    "# price 열의 데이터를 전처리\n",
    "for i in range(len(df)):\n",
    "    price_value = df.loc[i, 'price']\n",
    "    # 빈칸을 기준으로 앞부분만 남기고 나머지 제거\n",
    "    price_value = price_value.split(' ')[0]\n",
    "    # 숫자와 쉼표만 남기기\n",
    "    price_value = re.sub(r'[^\\d,]', '', price_value)\n",
    "    df.loc[i, 'price'] = price_value\n",
    "\n",
    "# color_options 비우기\n",
    "df['color_options'] = 'none'\n",
    "\n",
    "# category를 전부 'top'으로 바꾸기\n",
    "df['category'] = 'top'\n",
    "df.drop(columns=['size_options'],inplace=True)\n",
    "\n",
    "\n",
    "# 리뷰 데이터 전처리\n",
    "# reviews 컬럼의 JSON 데이터를 파싱하는 함수\n",
    "def parse_reviews(review_str):\n",
    "    try:\n",
    "        return json.loads(review_str.replace(\"'\", '\"'))\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "# 모든 리뷰를 담을 리스트\n",
    "all_reviews = []\n",
    "\n",
    "# 각 product_name별로 리뷰를 추출하여 통합\n",
    "for index, row in df.iterrows():\n",
    "    product_name = row['product_name']\n",
    "    reviews_list = parse_reviews(row['reviews'])\n",
    "    \n",
    "    for review in reviews_list:\n",
    "        review['product_name'] = product_name  # 각 리뷰에 제품 이름 추가\n",
    "        all_reviews.append(review)\n",
    "        \n",
    "# 통합 리뷰 데이터를 데이터프레임으로 변환\n",
    "reviews_df = pd.DataFrame(all_reviews)\n",
    "\n",
    "# weight, height, gender를 분리하여 새로운 컬럼 생성\n",
    "def split_weight_height_gender(whg):\n",
    "    parts = whg.split(' · ')\n",
    "    if len(parts) == 3:\n",
    "        gender, height, weight = parts\n",
    "        height = height.replace('cm', '')\n",
    "        weight = weight.replace('kg', '')\n",
    "        return gender.strip(), height.strip(), weight.strip()\n",
    "    return None, None, None\n",
    "\n",
    "reviews_df['gender'], reviews_df['height'], reviews_df['weight'] = zip(*reviews_df['weight_height_gender'].map(split_weight_height_gender))\n",
    "\n",
    "# weight_height_gender 컬럼 삭제\n",
    "reviews_df.drop(columns=['weight_height_gender'], inplace=True)\n",
    "\n",
    "# 컬럼명 변경\n",
    "reviews_df.rename(columns={'top_size': 'size_option', 'purchased_size': 'size'}, inplace=True)\n",
    "reviews_df.drop(columns=['purchased_product_id'],inplace=True)\n",
    "\n",
    "# 새로운 컬럼 top_size와 bottom_size 생성\n",
    "reviews_df['quality_comment'] = 'none'\n",
    "reviews_df['product_id'] = 'none'\n",
    "reviews_df['color'] = 'none'\n",
    "reviews_df['top_size'] = 'none'\n",
    "reviews_df['bottom_size'] = 'none'\n",
    "df.drop(columns=['reviews'],inplace=True)\n",
    "# 통합 리뷰 데이터를 CSV 파일로 저장\n",
    "reviews_df.to_csv('musinsa_reviews.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"Saved all reviews to musinsa_reviews.csv\")\n",
    "\n",
    "# 변경된 product 테이블도 CSV 파일로 저장\n",
    "df.to_csv('processed_products.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"Saved processed products to processed_products.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     review_id size_option brightness_comment color_comment thickness_comment  \\\n",
      "0    LV.4 유필리티          커요               어두워요           흐려요              두꺼워요   \n",
      "1  LV.5 용두산호카게       보통이에요              보통이에요         보통이에요             보통이에요   \n",
      "2  LV.7 홀리몰리롤리       보통이에요              보통이에요         보통이에요             보통이에요   \n",
      "3   LV.6 비종프리제       보통이에요              보통이에요         보통이에요             보통이에요   \n",
      "4     LV.8 빙하왕       보통이에요              보통이에요         보통이에요             보통이에요   \n",
      "\n",
      "  size                               comment            product_name gender  \\\n",
      "0   XL        작년에 삿던거 잘입고있어서 같은색 같은걸로 떠주문햇어요  헤비웨이트 오버사이즈 스웨트셔츠 [블랙]     남성   \n",
      "1    L              두껍기보단 딴딴해요 마치 제 실전압축근육처럼  헤비웨이트 오버사이즈 스웨트셔츠 [블랙]     남성   \n",
      "2    L  이원단에 2만원? 진짜 말도안되는 가성비템입니다 사랑해요 무탠다드  헤비웨이트 오버사이즈 스웨트셔츠 [블랙]     남성   \n",
      "3  2XL   좋아요 두깨감도 있어서 탄탄해서 오버하게\\n입을라고 크게 삿어요  헤비웨이트 오버사이즈 스웨트셔츠 [블랙]     남성   \n",
      "4    L         마음에 들어서 다른 색도 구매합니다 헤비웨이트 맛집!  헤비웨이트 오버사이즈 스웨트셔츠 [블랙]     남성   \n",
      "\n",
      "   height  weight top_size bottom_size  \n",
      "0   176.0    85.0     none        none  \n",
      "1   176.0    63.0     none        none  \n",
      "2   176.0    56.0     none        none  \n",
      "3   184.0    82.0     none        none  \n",
      "4   168.0    67.0     none        none  \n",
      "   rank product_id                product_name category   price  \\\n",
      "0     0       none      헤비웨이트 오버사이즈 스웨트셔츠 [블랙]      top  34,454   \n",
      "1     1       none        24S/S 오버핏 피케티셔츠 (블랙)      top  34,738   \n",
      "2     2       none           화란 세미오버 니트 모카 베이지      top  66,076   \n",
      "3     3       none  오버사이즈 집업 카라 터틀넥 니트 [BLACK]      top  52,600   \n",
      "4     4       none      460G 컷 헤비 피그먼트 티셔츠-차콜-      top  36,600   \n",
      "\n",
      "                                           image_url  \\\n",
      "0  https://image.msscdn.net/images/goods_img/2020...   \n",
      "1  https://image.msscdn.net/images/goods_img/2019...   \n",
      "2  https://image.msscdn.net/images/goods_img/2019...   \n",
      "3  https://image.msscdn.net/images/goods_img/2021...   \n",
      "4  https://image.msscdn.net/images/goods_img/2021...   \n",
      "\n",
      "                                 description  \\\n",
      "0  https://www.musinsa.com/app/goods/1642887   \n",
      "1  https://www.musinsa.com/app/goods/1031260   \n",
      "2  https://www.musinsa.com/app/goods/1139099   \n",
      "3  https://www.musinsa.com/app/goods/2226766   \n",
      "4  https://www.musinsa.com/app/goods/1944612   \n",
      "\n",
      "                                         size color_options  \n",
      "0         ['S', 'M', 'L', 'XL', '2XL', '3XL']          none  \n",
      "1                ['S', 'M', 'L', 'XL', 'XXL']          none  \n",
      "2  ['42 (FOR WOMAN)', '44', '46', '48', '50']          none  \n",
      "3                      ['WS', 'M', 'L', 'XL']          none  \n",
      "4                       ['S', 'M', 'L', 'XL']          none  \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "이 코드는 내 전처리 파트임.\n",
    "'''\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "\n",
    "# AWS S3 버킷 정보\n",
    "bucket_name = 'papalio-test-bucket'\n",
    "file_key2 = 'test_otto/musinsa_reviews.csv'\n",
    "file_key3 = 'test_otto/processed_products.csv'\n",
    "\n",
    "# S3 클라이언트 생성\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3에서 CSV 파일 다운로드\n",
    "try:\n",
    "    # 첫 번째 파일 다운로드 및 DataFrame으로 변환\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=file_key2)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    df = pd.read_csv(io.StringIO(content))\n",
    "    \n",
    "    # 두 번째 파일 다운로드 및 DataFrame으로 변환\n",
    "    response2 = s3.get_object(Bucket=bucket_name, Key=file_key3)\n",
    "    content2 = response2['Body'].read().decode('utf-8')\n",
    "    df2 = pd.read_csv(io.StringIO(content2))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reading S3 object: {e}\")\n",
    "\n",
    "# 데이터 출력\n",
    "print(df.head())\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder non-integrated-data created in bucket otto-glue\n",
      "Uploaded 29cm_products.csv to otto-glue/non-integrated-data/29cm_products.csv\n",
      "Uploaded 29cm_reviews.csv to otto-glue/non-integrated-data/29cm_reviews.csv\n",
      "Uploaded processed_products.csv to otto-glue/non-integrated-data/processed_products.csv\n",
      "Uploaded musinsa_reviews.csv to otto-glue/non-integrated-data/musinsa_reviews.csv\n",
      "Uploaded zigzag_product_infos.csv to otto-glue/non-integrated-data/zigzag_product_infos.csv\n",
      "Uploaded zigzag_reviews.csv to otto-glue/non-integrated-data/zigzag_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# AWS 자격 증명 설정\n",
    "aws_access_key_id = 'AKIA4RRVVY55QSQPIXPQ'\n",
    "aws_secret_access_key = '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'\n",
    "\n",
    "# S3 클라이언트 생성\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "\n",
    "bucket_name = 'otto-glue'\n",
    "folder_name = 'non-integrated-data'\n",
    "local_files = [\n",
    "    '29cm_products.csv',\n",
    "    '29cm_reviews.csv',\n",
    "    'processed_products.csv',\n",
    "    'musinsa_reviews.csv',\n",
    "    'zigzag_product_infos.csv',\n",
    "    'zigzag_reviews.csv'\n",
    "]\n",
    "\n",
    "def upload_files_to_s3(bucket_name, folder_name, local_files):\n",
    "    for file_name in local_files:\n",
    "        try:\n",
    "            s3_client.upload_file(file_name, bucket_name, f\"{folder_name}/{file_name}\")\n",
    "            print(f\"Uploaded {file_name} to {bucket_name}/{folder_name}/{file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {file_name}: {e}\")\n",
    "\n",
    "# 폴더 만들기\n",
    "def create_s3_folder(bucket_name, folder_name):\n",
    "    try:\n",
    "        s3_client.put_object(Bucket=bucket_name, Key=(folder_name+'/'))\n",
    "        print(f\"Folder {folder_name} created in bucket {bucket_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating folder {folder_name}: {e}\")\n",
    "\n",
    "# 실행\n",
    "create_s3_folder(bucket_name, folder_name)\n",
    "upload_files_to_s3(bucket_name, folder_name, local_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Reviews DataFrame Columns:\n",
      "Index(['review_id', 'product_name', 'color', 'size', 'height', 'gender',\n",
      "       'weight', 'top_size', 'bottom_size', 'size_comment', 'quality_comment',\n",
      "       'color_comment', 'thickness_comment', 'brightness_comment', 'comment',\n",
      "       'product_name112538672_1', '112538672_1', '112538672', '(롱롱ver)블랙',\n",
      "       'one size', '정사이즈예요', '아주 만족해요', '화면과 비슷해요', '156cm', '50kg',\n",
      "       '이가격에, 이 옵션 수에, 이런 총알배송??? 만족도 최상. 당장 사십시오. 저는 157센치 스펙에, 화이트 롱 ver. 구매했어요 (근데 너무 벌써 뽕을 뽑아서 블랙 롱롱 ver. 추가 구매함) 패드 안비쳐요 💥 패드 안비침, 실존. \\n캡모양 로켓발…\\n더보기',\n",
      "       '상의 55', 'none', '[무료배송][3만장판매] 티버 와이드 코튼 셔츠'],\n",
      "      dtype='object')\n",
      "Combined Products DataFrame Columns:\n",
      "Index(['product_id', 'rank', 'product_name', 'category', 'price', 'image_url',\n",
      "       'description', 'color', 'size', 'platform'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import random\n",
    "\n",
    "# AWS 자격 증명 설정\n",
    "aws_access_key_id = 'A-PQ'\n",
    "aws_secret_access_key = '-'\n",
    "\n",
    "# S3 클라이언트 생성\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "\n",
    "color_list = [\"red\", \"blue\", \"green\", \"yellow\", \"black\", \"white\", \"gray\", \"orange\", \"purple\", \"brown\",\n",
    "              \"pink\", \"cyan\", \"magenta\", \"lime\", \"teal\", \"lavender\", \"beige\", \"maroon\", \"olive\", \"navy\"]\n",
    "\n",
    "bucket_name = 'otto-glue'\n",
    "folder_name = 'non-integrated-data'\n",
    "\n",
    "product_file_names = [\n",
    "    '29cm_products.csv',\n",
    "    'processed_products.csv',\n",
    "    'zigzag_products.csv',\n",
    "]\n",
    "review_file_names = [\n",
    "    '29cm_reviews.csv',\n",
    "    'musinsa_reviews.csv',\n",
    "    'zigzag_reviews.csv'\n",
    "]\n",
    "\n",
    "def read_s3_csv(bucket_name, file_key):\n",
    "    obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    df = pd.read_csv(obj['Body'])\n",
    "    return df\n",
    "\n",
    "def process_reviews(df, is_zigzag=False):\n",
    "    if is_zigzag:\n",
    "        df.drop(columns=['Unnamed: 0', 'product_id'], inplace=True, errors='ignore')\n",
    "        df['brightness_comment'] = 'none'\n",
    "        df['gender'] = 'none'\n",
    "        df['thickness_comment'] = 'none'\n",
    "    return df\n",
    "\n",
    "def process_products(df, platform_name, is_zigzag=False):\n",
    "    if is_zigzag:\n",
    "        df.drop(columns=['Unnamed: 0'], inplace=True, errors='ignore')\n",
    "    df['platform'] = platform_name\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # 리뷰 파일 통합\n",
    "    review_dfs = []\n",
    "    for file_name in review_file_names:\n",
    "        file_key = f'{folder_name}/{file_name}'\n",
    "        df = read_s3_csv(bucket_name, file_key)\n",
    "        if 'zigzag' in file_name:\n",
    "            df = process_reviews(df, is_zigzag=True)\n",
    "        review_dfs.append(df)\n",
    "\n",
    "    combined_reviews_df = pd.concat(review_dfs).drop_duplicates().reset_index(drop=True)\n",
    "    print(\"Combined Reviews DataFrame Columns:\")\n",
    "    print(combined_reviews_df.columns)\n",
    "\n",
    "    # 제품 파일 통합\n",
    "    product_dfs = []\n",
    "    platform_mapping = {\n",
    "        '29cm_products.csv': '29cm',\n",
    "        'processed_products.csv': 'musinsa',\n",
    "        'zigzag_products.csv': 'zigzag'\n",
    "    }\n",
    "\n",
    "    for file_name in product_file_names:\n",
    "        file_key = f'{folder_name}/{file_name}'\n",
    "        df = read_s3_csv(bucket_name, file_key)\n",
    "        platform_name = platform_mapping[file_name]\n",
    "        if 'zigzag' in file_name:\n",
    "            df = process_products(df, platform_name, is_zigzag=True)\n",
    "        else:\n",
    "            df = process_products(df, platform_name)\n",
    "        product_dfs.append(df)\n",
    "\n",
    "    combined_products_df = pd.concat(product_dfs).drop_duplicates().reset_index(drop=True)\n",
    "    print(\"Combined Products DataFrame Columns:\")\n",
    "    print(combined_products_df.columns)\n",
    "\n",
    "    # 색상 추가\n",
    "    def add_random_colors(row):\n",
    "        if row['color'] == 'none':\n",
    "            num_colors = random.randint(1, 4)\n",
    "            row['color'] = ', '.join(random.sample(color_list, num_colors))\n",
    "        return row\n",
    "\n",
    "    combined_products_df = combined_products_df.apply(add_random_colors, axis=1)\n",
    "\n",
    "    # CSV 파일 저장\n",
    "    combined_reviews_df.to_csv('combined_reviews.csv', index=False, encoding='utf-8-sig')\n",
    "    combined_products_df.to_csv('combined_products.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['review_id', 'product_id', 'color', 'size', 'size_comment', 'quality_comment', 'color_comment', 'height', 'weight', 'comment', 'top_size', 'bottom_size', 'product_name']\n",
      "['product_id', 'category', 'description', 'product_name', 'price', 'image_url', 'size', 'color', 'rank']\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# AWS 자격 증명 설정\n",
    "aws_access_key_id = 'PQ'\n",
    "aws_secret_access_key = '6'\n",
    "\n",
    "# S3 클라이언트 생성\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "\n",
    "bucket_name = 'otto-glue'\n",
    "folder_name = 'integrated-data'\n",
    "\n",
    "combined_reviews_file_key = 'integrated-data/combined_reviews.csv'\n",
    "combined_products_file_key = 'integrated-data/combined_products.csv'\n",
    "test_key = 'non-integrated-data/zigzag_reviews.csv'\n",
    "test_key2 = 'non-integrated-data/zigzag_products.csv'\n",
    "\n",
    "def read_s3_csv(bucket_name, file_key):\n",
    "    obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    df = pd.read_csv(obj['Body'])\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # combined_reviews.csv 불러오기\n",
    "    #combined_reviews_df = read_s3_csv(bucket_name, combined_reviews_file_key)\n",
    "    #print(\"Combined Reviews DataFrame Columns:\")\n",
    "    #print(combined_reviews_df.columns.tolist())\n",
    "\n",
    "    # combined_products.csv 불러오기\n",
    "    #combined_products_df = read_s3_csv(bucket_name, combined_products_file_key)\n",
    "    #print(\"Combined Products DataFrame Columns:\")\n",
    "    #print(combined_products_df.columns.tolist())\n",
    "    \n",
    "    test_df = read_s3_csv(bucket_name, test_key)\n",
    "    test_df2 = read_s3_csv(bucket_name, test_key2)\n",
    "    print(test_df.columns.tolist())\n",
    "    print(test_df2.columns.tolist())\n",
    "    #combined_reviews_df.to_csv('combined_reviews.csv', index=False, encoding='utf-8-sig')\n",
    "    test_df.to_csv('test_zigzag_reviews.csv', index=False, encoding='utf-8-sig')\n",
    "    test_df2.to_csv('test_zigzag_product.csv', index=False, encoding='utf-8-sig' )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자열 변환 :  2024-07-25 14:49:32\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "\n",
    "print(\"문자열 변환 : \", now.strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
