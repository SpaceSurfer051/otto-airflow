{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Using cached boto3-1.34.145-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: botocore in c:\\users\\dldud\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.34.29)\n",
      "Collecting s3transfer\n",
      "  Using cached s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting botocore\n",
      "  Using cached botocore-1.34.145-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\dldud\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\dldud\\appdata\\roaming\\python\\python311\\site-packages (from botocore) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\dldud\\appdata\\roaming\\python\\python311\\site-packages (from botocore) (1.26.18)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dldud\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.16.0)\n",
      "Using cached boto3-1.34.145-py3-none-any.whl (139 kB)\n",
      "Using cached botocore-1.34.145-py3-none-any.whl (12.4 MB)\n",
      "Using cached s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "Installing collected packages: botocore, s3transfer, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.34.29\n",
      "    Uninstalling botocore-1.34.29:\n",
      "      Successfully uninstalled botocore-1.34.29\n",
      "Successfully installed boto3-1.34.145 botocore-1.34.145 s3transfer-0.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awsebcli 3.20.10 requires botocore<1.32.0,>1.23.41, but you have botocore 1.34.145 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install boto3 botocore s3transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "\n",
    "# AWS S3 ë²„í‚· ì •ë³´\n",
    "bucket_name = 'papalio-test-bucket'\n",
    "file_key = 'test_otto/products_with_size_color.csv'\n",
    "\n",
    "# S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3ì—ì„œ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
    "try:\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    \n",
    "    # CSV ë‚´ìš©ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "    df = pd.read_csv(io.StringIO(content))\n",
    "    \n",
    "    # ë°ì´í„°í”„ë ˆì„ ì¶œë ¥\n",
    "    #print(df)\n",
    "    #df csvíŒŒì¼ë¡œ ë§Œë“¤ê¸°\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading S3 object: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "\n",
    "'''\n",
    "ì´ ë‹¨ì€ product tableì„ ì „ì²˜ë¦¬í•˜ëŠ” ë¶€ë¶„ì„\n",
    "'''\n",
    "\n",
    "#production preprocessing\n",
    "df['product_id'] = df.index.to_series().apply(lambda x: f'top_{x}')\n",
    "# price ì—´ì˜ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬\n",
    "for i in range(len(df)):\n",
    "    price_value = df.loc[i, 'price']\n",
    "    # ë¹ˆì¹¸ì„ ê¸°ì¤€ìœ¼ë¡œ ì•ë¶€ë¶„ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì œê±°\n",
    "    price_value = price_value.split(' ')[0]\n",
    "    # ìˆ«ìì™€ ì‰¼í‘œë§Œ ë‚¨ê¸°ê¸°\n",
    "    price_value = re.sub(r'[^\\d,]', '', price_value)\n",
    "    df.loc[i, 'price'] = price_value\n",
    "\n",
    "\n",
    "#color_options ë¹„ìš°ê¸°\n",
    "\n",
    "#categoryë¥¼ ì „ë¶€ topìœ¼ë¡œ ë°”ê¾¸ê¸°\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"preprocessing_test.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 549 (char 548)\n",
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 1827 (char 1826)\n",
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 436 (char 435)\n",
      "Error decoding JSON: Invalid \\escape: line 1 column 1440 (char 1439)\n",
      "Saved all reviews to musinsa_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# CSV íŒŒì¼ ì½ê¸°\n",
    "df = pd.read_csv('preprocessing_test.csv')\n",
    "\n",
    "# reviews ì»¬ëŸ¼ì˜ JSON ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜\n",
    "def parse_reviews(review_str):\n",
    "    try:\n",
    "        return json.loads(review_str.replace(\"'\", '\"'))\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "# ëª¨ë“  ë¦¬ë·°ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸\n",
    "all_reviews = []\n",
    "\n",
    "# ê° product_nameë³„ë¡œ ë¦¬ë·°ë¥¼ ì¶”ì¶œí•˜ì—¬ í†µí•©\n",
    "for index, row in df.iterrows():\n",
    "    product_name = row['product_name']\n",
    "    reviews_list = parse_reviews(row['reviews'])\n",
    "    \n",
    "    for review in reviews_list:\n",
    "        review['product_name'] = product_name  # ê° ë¦¬ë·°ì— ì œí’ˆ ì´ë¦„ ì¶”ê°€\n",
    "        all_reviews.append(review)\n",
    "        \n",
    "# í†µí•© ë¦¬ë·° ë°ì´í„°ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "reviews_df = pd.DataFrame(all_reviews)\n",
    "\n",
    "# weight, height, genderë¥¼ ë¶„ë¦¬í•˜ì—¬ ìƒˆë¡œìš´ ì»¬ëŸ¼ ìƒì„±\n",
    "def split_weight_height_gender(whg):\n",
    "    parts = whg.split(' Â· ')\n",
    "    if len(parts) == 3:\n",
    "        gender, height, weight = parts\n",
    "        height = height.replace('cm', '')\n",
    "        weight = weight.replace('kg', '')\n",
    "        return gender.strip(), height.strip(), weight.strip()\n",
    "    return None, None, None\n",
    "\n",
    "reviews_df['gender'], reviews_df['height'], reviews_df['weight'] = zip(*reviews_df['weight_height_gender'].map(split_weight_height_gender))\n",
    "\n",
    "# ì»¬ëŸ¼ëª… ë³€ê²½\n",
    "reviews_df.rename(columns={'top_size': 'size_option', 'purchased_size': 'size'}, inplace=True)\n",
    "\n",
    "# ìƒˆë¡œìš´ ì»¬ëŸ¼ top_sizeì™€ bottom_size ìƒì„±\n",
    "reviews_df['top_size'] = 'none'\n",
    "reviews_df['bottom_size'] = 'none'\n",
    "# ì»¬ëŸ¼ ì‚­ì œ\n",
    "reviews_df.drop(columns=['weight_height_gender'], inplace=True)\n",
    "\n",
    "# í†µí•© ë¦¬ë·° ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "reviews_df.to_csv('musinsa_reviews.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"Saved all reviews to musinsa_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 549 (char 548)\n",
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 1827 (char 1826)\n",
      "Error decoding JSON: Expecting ',' delimiter: line 1 column 436 (char 435)\n",
      "Error decoding JSON: Invalid \\escape: line 1 column 1440 (char 1439)\n",
      "Saved all reviews to musinsa_reviews.csv\n",
      "Saved processed products to processed_products.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ì´ ì½”ë“œëŠ” ë‚´ ì „ì²˜ë¦¬ íŒŒíŠ¸ì„.\n",
    "'''\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "\n",
    "# AWS S3 ë²„í‚· ì •ë³´\n",
    "bucket_name = 'papalio-test-bucket'\n",
    "file_key = 'test_otto/products_with_size_color.csv'\n",
    "\n",
    "# S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3ì—ì„œ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
    "try:\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    \n",
    "    # CSV ë‚´ìš©ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "    df = pd.read_csv(io.StringIO(content))\n",
    "    \n",
    "    # ë°ì´í„°í”„ë ˆì„ ì¶œë ¥\n",
    "    #print(df)\n",
    "    #df csvíŒŒì¼ë¡œ ë§Œë“¤ê¸°\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading S3 object: {e}\")\n",
    "\n",
    "# Production preprocessing\n",
    "df['product_id'] = 'none'\n",
    "df.insert(0, 'rank', [i for i in range(0, len(df))])\n",
    "\n",
    "# price ì—´ì˜ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬\n",
    "for i in range(len(df)):\n",
    "    price_value = df.loc[i, 'price']\n",
    "    # ë¹ˆì¹¸ì„ ê¸°ì¤€ìœ¼ë¡œ ì•ë¶€ë¶„ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì œê±°\n",
    "    price_value = price_value.split(' ')[0]\n",
    "    # ìˆ«ìì™€ ì‰¼í‘œë§Œ ë‚¨ê¸°ê¸°\n",
    "    price_value = re.sub(r'[^\\d,]', '', price_value)\n",
    "    df.loc[i, 'price'] = price_value\n",
    "\n",
    "# color_options ë¹„ìš°ê¸°\n",
    "df['color_options'] = 'none'\n",
    "\n",
    "# categoryë¥¼ ì „ë¶€ 'top'ìœ¼ë¡œ ë°”ê¾¸ê¸°\n",
    "df['category'] = 'top'\n",
    "df.drop(columns=['size_options'],inplace=True)\n",
    "\n",
    "\n",
    "# ë¦¬ë·° ë°ì´í„° ì „ì²˜ë¦¬\n",
    "# reviews ì»¬ëŸ¼ì˜ JSON ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜\n",
    "def parse_reviews(review_str):\n",
    "    try:\n",
    "        return json.loads(review_str.replace(\"'\", '\"'))\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "# ëª¨ë“  ë¦¬ë·°ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸\n",
    "all_reviews = []\n",
    "\n",
    "# ê° product_nameë³„ë¡œ ë¦¬ë·°ë¥¼ ì¶”ì¶œí•˜ì—¬ í†µí•©\n",
    "for index, row in df.iterrows():\n",
    "    product_name = row['product_name']\n",
    "    reviews_list = parse_reviews(row['reviews'])\n",
    "    \n",
    "    for review in reviews_list:\n",
    "        review['product_name'] = product_name  # ê° ë¦¬ë·°ì— ì œí’ˆ ì´ë¦„ ì¶”ê°€\n",
    "        all_reviews.append(review)\n",
    "        \n",
    "# í†µí•© ë¦¬ë·° ë°ì´í„°ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "reviews_df = pd.DataFrame(all_reviews)\n",
    "\n",
    "# weight, height, genderë¥¼ ë¶„ë¦¬í•˜ì—¬ ìƒˆë¡œìš´ ì»¬ëŸ¼ ìƒì„±\n",
    "def split_weight_height_gender(whg):\n",
    "    parts = whg.split(' Â· ')\n",
    "    if len(parts) == 3:\n",
    "        gender, height, weight = parts\n",
    "        height = height.replace('cm', '')\n",
    "        weight = weight.replace('kg', '')\n",
    "        return gender.strip(), height.strip(), weight.strip()\n",
    "    return None, None, None\n",
    "\n",
    "reviews_df['gender'], reviews_df['height'], reviews_df['weight'] = zip(*reviews_df['weight_height_gender'].map(split_weight_height_gender))\n",
    "\n",
    "# weight_height_gender ì»¬ëŸ¼ ì‚­ì œ\n",
    "reviews_df.drop(columns=['weight_height_gender'], inplace=True)\n",
    "\n",
    "# ì»¬ëŸ¼ëª… ë³€ê²½\n",
    "reviews_df.rename(columns={'top_size': 'size_option', 'purchased_size': 'size'}, inplace=True)\n",
    "reviews_df.drop(columns=['purchased_product_id'],inplace=True)\n",
    "\n",
    "# ìƒˆë¡œìš´ ì»¬ëŸ¼ top_sizeì™€ bottom_size ìƒì„±\n",
    "reviews_df['quality_comment'] = 'none'\n",
    "reviews_df['product_id'] = 'none'\n",
    "reviews_df['color'] = 'none'\n",
    "reviews_df['top_size'] = 'none'\n",
    "reviews_df['bottom_size'] = 'none'\n",
    "df.drop(columns=['reviews'],inplace=True)\n",
    "# í†µí•© ë¦¬ë·° ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "reviews_df.to_csv('musinsa_reviews.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"Saved all reviews to musinsa_reviews.csv\")\n",
    "\n",
    "# ë³€ê²½ëœ product í…Œì´ë¸”ë„ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "df.to_csv('processed_products.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"Saved processed products to processed_products.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     review_id size_option brightness_comment color_comment thickness_comment  \\\n",
      "0    LV.4 ìœ í•„ë¦¬í‹°          ì»¤ìš”               ì–´ë‘ì›Œìš”           íë ¤ìš”              ë‘êº¼ì›Œìš”   \n",
      "1  LV.5 ìš©ë‘ì‚°í˜¸ì¹´ê²Œ       ë³´í†µì´ì—ìš”              ë³´í†µì´ì—ìš”         ë³´í†µì´ì—ìš”             ë³´í†µì´ì—ìš”   \n",
      "2  LV.7 í™€ë¦¬ëª°ë¦¬ë¡¤ë¦¬       ë³´í†µì´ì—ìš”              ë³´í†µì´ì—ìš”         ë³´í†µì´ì—ìš”             ë³´í†µì´ì—ìš”   \n",
      "3   LV.6 ë¹„ì¢…í”„ë¦¬ì œ       ë³´í†µì´ì—ìš”              ë³´í†µì´ì—ìš”         ë³´í†µì´ì—ìš”             ë³´í†µì´ì—ìš”   \n",
      "4     LV.8 ë¹™í•˜ì™•       ë³´í†µì´ì—ìš”              ë³´í†µì´ì—ìš”         ë³´í†µì´ì—ìš”             ë³´í†µì´ì—ìš”   \n",
      "\n",
      "  size                               comment            product_name gender  \\\n",
      "0   XL        ì‘ë…„ì— ì‚¿ë˜ê±° ì˜ì…ê³ ìˆì–´ì„œ ê°™ì€ìƒ‰ ê°™ì€ê±¸ë¡œ ë– ì£¼ë¬¸í–‡ì–´ìš”  í—¤ë¹„ì›¨ì´íŠ¸ ì˜¤ë²„ì‚¬ì´ì¦ˆ ìŠ¤ì›¨íŠ¸ì…”ì¸  [ë¸”ë™]     ë‚¨ì„±   \n",
      "1    L              ë‘ê»ê¸°ë³´ë‹¨ ë”´ë”´í•´ìš” ë§ˆì¹˜ ì œ ì‹¤ì „ì••ì¶•ê·¼ìœ¡ì²˜ëŸ¼  í—¤ë¹„ì›¨ì´íŠ¸ ì˜¤ë²„ì‚¬ì´ì¦ˆ ìŠ¤ì›¨íŠ¸ì…”ì¸  [ë¸”ë™]     ë‚¨ì„±   \n",
      "2    L  ì´ì›ë‹¨ì— 2ë§Œì›? ì§„ì§œ ë§ë„ì•ˆë˜ëŠ” ê°€ì„±ë¹„í…œì…ë‹ˆë‹¤ ì‚¬ë‘í•´ìš” ë¬´íƒ ë‹¤ë“œ  í—¤ë¹„ì›¨ì´íŠ¸ ì˜¤ë²„ì‚¬ì´ì¦ˆ ìŠ¤ì›¨íŠ¸ì…”ì¸  [ë¸”ë™]     ë‚¨ì„±   \n",
      "3  2XL   ì¢‹ì•„ìš” ë‘ê¹¨ê°ë„ ìˆì–´ì„œ íƒ„íƒ„í•´ì„œ ì˜¤ë²„í•˜ê²Œ\\nì…ì„ë¼ê³  í¬ê²Œ ì‚¿ì–´ìš”  í—¤ë¹„ì›¨ì´íŠ¸ ì˜¤ë²„ì‚¬ì´ì¦ˆ ìŠ¤ì›¨íŠ¸ì…”ì¸  [ë¸”ë™]     ë‚¨ì„±   \n",
      "4    L         ë§ˆìŒì— ë“¤ì–´ì„œ ë‹¤ë¥¸ ìƒ‰ë„ êµ¬ë§¤í•©ë‹ˆë‹¤ í—¤ë¹„ì›¨ì´íŠ¸ ë§›ì§‘!  í—¤ë¹„ì›¨ì´íŠ¸ ì˜¤ë²„ì‚¬ì´ì¦ˆ ìŠ¤ì›¨íŠ¸ì…”ì¸  [ë¸”ë™]     ë‚¨ì„±   \n",
      "\n",
      "   height  weight top_size bottom_size  \n",
      "0   176.0    85.0     none        none  \n",
      "1   176.0    63.0     none        none  \n",
      "2   176.0    56.0     none        none  \n",
      "3   184.0    82.0     none        none  \n",
      "4   168.0    67.0     none        none  \n",
      "   rank product_id                product_name category   price  \\\n",
      "0     0       none      í—¤ë¹„ì›¨ì´íŠ¸ ì˜¤ë²„ì‚¬ì´ì¦ˆ ìŠ¤ì›¨íŠ¸ì…”ì¸  [ë¸”ë™]      top  34,454   \n",
      "1     1       none        24S/S ì˜¤ë²„í• í”¼ì¼€í‹°ì…”ì¸  (ë¸”ë™)      top  34,738   \n",
      "2     2       none           í™”ë€ ì„¸ë¯¸ì˜¤ë²„ ë‹ˆíŠ¸ ëª¨ì¹´ ë² ì´ì§€      top  66,076   \n",
      "3     3       none  ì˜¤ë²„ì‚¬ì´ì¦ˆ ì§‘ì—… ì¹´ë¼ í„°í‹€ë„¥ ë‹ˆíŠ¸ [BLACK]      top  52,600   \n",
      "4     4       none      460G ì»· í—¤ë¹„ í”¼ê·¸ë¨¼íŠ¸ í‹°ì…”ì¸ -ì°¨ì½œ-      top  36,600   \n",
      "\n",
      "                                           image_url  \\\n",
      "0  https://image.msscdn.net/images/goods_img/2020...   \n",
      "1  https://image.msscdn.net/images/goods_img/2019...   \n",
      "2  https://image.msscdn.net/images/goods_img/2019...   \n",
      "3  https://image.msscdn.net/images/goods_img/2021...   \n",
      "4  https://image.msscdn.net/images/goods_img/2021...   \n",
      "\n",
      "                                 description  \\\n",
      "0  https://www.musinsa.com/app/goods/1642887   \n",
      "1  https://www.musinsa.com/app/goods/1031260   \n",
      "2  https://www.musinsa.com/app/goods/1139099   \n",
      "3  https://www.musinsa.com/app/goods/2226766   \n",
      "4  https://www.musinsa.com/app/goods/1944612   \n",
      "\n",
      "                                         size color_options  \n",
      "0         ['S', 'M', 'L', 'XL', '2XL', '3XL']          none  \n",
      "1                ['S', 'M', 'L', 'XL', 'XXL']          none  \n",
      "2  ['42 (FOR WOMAN)', '44', '46', '48', '50']          none  \n",
      "3                      ['WS', 'M', 'L', 'XL']          none  \n",
      "4                       ['S', 'M', 'L', 'XL']          none  \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ì´ ì½”ë“œëŠ” ë‚´ ì „ì²˜ë¦¬ íŒŒíŠ¸ì„.\n",
    "'''\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "\n",
    "# AWS S3 ë²„í‚· ì •ë³´\n",
    "bucket_name = 'papalio-test-bucket'\n",
    "file_key2 = 'test_otto/musinsa_reviews.csv'\n",
    "file_key3 = 'test_otto/processed_products.csv'\n",
    "\n",
    "# S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3ì—ì„œ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
    "try:\n",
    "    # ì²« ë²ˆì§¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=file_key2)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    df = pd.read_csv(io.StringIO(content))\n",
    "    \n",
    "    # ë‘ ë²ˆì§¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "    response2 = s3.get_object(Bucket=bucket_name, Key=file_key3)\n",
    "    content2 = response2['Body'].read().decode('utf-8')\n",
    "    df2 = pd.read_csv(io.StringIO(content2))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reading S3 object: {e}\")\n",
    "\n",
    "# ë°ì´í„° ì¶œë ¥\n",
    "print(df.head())\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder non-integrated-data created in bucket otto-glue\n",
      "Uploaded 29cm_products.csv to otto-glue/non-integrated-data/29cm_products.csv\n",
      "Uploaded 29cm_reviews.csv to otto-glue/non-integrated-data/29cm_reviews.csv\n",
      "Uploaded processed_products.csv to otto-glue/non-integrated-data/processed_products.csv\n",
      "Uploaded musinsa_reviews.csv to otto-glue/non-integrated-data/musinsa_reviews.csv\n",
      "Uploaded zigzag_product_infos.csv to otto-glue/non-integrated-data/zigzag_product_infos.csv\n",
      "Uploaded zigzag_reviews.csv to otto-glue/non-integrated-data/zigzag_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# AWS ìê²© ì¦ëª… ì„¤ì •\n",
    "aws_access_key_id = 'AKIA4RRVVY55QSQPIXPQ'\n",
    "aws_secret_access_key = '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'\n",
    "\n",
    "# S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "\n",
    "bucket_name = 'otto-glue'\n",
    "folder_name = 'non-integrated-data'\n",
    "local_files = [\n",
    "    '29cm_products.csv',\n",
    "    '29cm_reviews.csv',\n",
    "    'processed_products.csv',\n",
    "    'musinsa_reviews.csv',\n",
    "    'zigzag_product_infos.csv',\n",
    "    'zigzag_reviews.csv'\n",
    "]\n",
    "\n",
    "def upload_files_to_s3(bucket_name, folder_name, local_files):\n",
    "    for file_name in local_files:\n",
    "        try:\n",
    "            s3_client.upload_file(file_name, bucket_name, f\"{folder_name}/{file_name}\")\n",
    "            print(f\"Uploaded {file_name} to {bucket_name}/{folder_name}/{file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {file_name}: {e}\")\n",
    "\n",
    "# í´ë” ë§Œë“¤ê¸°\n",
    "def create_s3_folder(bucket_name, folder_name):\n",
    "    try:\n",
    "        s3_client.put_object(Bucket=bucket_name, Key=(folder_name+'/'))\n",
    "        print(f\"Folder {folder_name} created in bucket {bucket_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating folder {folder_name}: {e}\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "create_s3_folder(bucket_name, folder_name)\n",
    "upload_files_to_s3(bucket_name, folder_name, local_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Reviews DataFrame Columns:\n",
      "Index(['review_id', 'product_name', 'color', 'size', 'height', 'gender',\n",
      "       'weight', 'top_size', 'bottom_size', 'size_comment', 'quality_comment',\n",
      "       'color_comment', 'thickness_comment', 'brightness_comment', 'comment',\n",
      "       'product_name112538672_1', '112538672_1', '112538672', '(ë¡±ë¡±ver)ë¸”ë™',\n",
      "       'one size', 'ì •ì‚¬ì´ì¦ˆì˜ˆìš”', 'ì•„ì£¼ ë§Œì¡±í•´ìš”', 'í™”ë©´ê³¼ ë¹„ìŠ·í•´ìš”', '156cm', '50kg',\n",
      "       'ì´ê°€ê²©ì—, ì´ ì˜µì…˜ ìˆ˜ì—, ì´ëŸ° ì´ì•Œë°°ì†¡??? ë§Œì¡±ë„ ìµœìƒ. ë‹¹ì¥ ì‚¬ì‹­ì‹œì˜¤. ì €ëŠ” 157ì„¼ì¹˜ ìŠ¤í™ì—, í™”ì´íŠ¸ ë¡± ver. êµ¬ë§¤í–ˆì–´ìš” (ê·¼ë° ë„ˆë¬´ ë²Œì¨ ë½•ì„ ë½‘ì•„ì„œ ë¸”ë™ ë¡±ë¡± ver. ì¶”ê°€ êµ¬ë§¤í•¨) íŒ¨ë“œ ì•ˆë¹„ì³ìš” ğŸ’¥ íŒ¨ë“œ ì•ˆë¹„ì¹¨, ì‹¤ì¡´. \\nìº¡ëª¨ì–‘ ë¡œì¼“ë°œâ€¦\\në”ë³´ê¸°',\n",
      "       'ìƒì˜ 55', 'none', '[ë¬´ë£Œë°°ì†¡][3ë§Œì¥íŒë§¤] í‹°ë²„ ì™€ì´ë“œ ì½”íŠ¼ ì…”ì¸ '],\n",
      "      dtype='object')\n",
      "Combined Products DataFrame Columns:\n",
      "Index(['product_id', 'rank', 'product_name', 'category', 'price', 'image_url',\n",
      "       'description', 'color', 'size', 'platform'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import random\n",
    "\n",
    "# AWS ìê²© ì¦ëª… ì„¤ì •\n",
    "aws_access_key_id = 'A-PQ'\n",
    "aws_secret_access_key = '-'\n",
    "\n",
    "# S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "\n",
    "color_list = [\"red\", \"blue\", \"green\", \"yellow\", \"black\", \"white\", \"gray\", \"orange\", \"purple\", \"brown\",\n",
    "              \"pink\", \"cyan\", \"magenta\", \"lime\", \"teal\", \"lavender\", \"beige\", \"maroon\", \"olive\", \"navy\"]\n",
    "\n",
    "bucket_name = 'otto-glue'\n",
    "folder_name = 'non-integrated-data'\n",
    "\n",
    "product_file_names = [\n",
    "    '29cm_products.csv',\n",
    "    'processed_products.csv',\n",
    "    'zigzag_products.csv',\n",
    "]\n",
    "review_file_names = [\n",
    "    '29cm_reviews.csv',\n",
    "    'musinsa_reviews.csv',\n",
    "    'zigzag_reviews.csv'\n",
    "]\n",
    "\n",
    "def read_s3_csv(bucket_name, file_key):\n",
    "    obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    df = pd.read_csv(obj['Body'])\n",
    "    return df\n",
    "\n",
    "def process_reviews(df, is_zigzag=False):\n",
    "    if is_zigzag:\n",
    "        df.drop(columns=['Unnamed: 0', 'product_id'], inplace=True, errors='ignore')\n",
    "        df['brightness_comment'] = 'none'\n",
    "        df['gender'] = 'none'\n",
    "        df['thickness_comment'] = 'none'\n",
    "    return df\n",
    "\n",
    "def process_products(df, platform_name, is_zigzag=False):\n",
    "    if is_zigzag:\n",
    "        df.drop(columns=['Unnamed: 0'], inplace=True, errors='ignore')\n",
    "    df['platform'] = platform_name\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # ë¦¬ë·° íŒŒì¼ í†µí•©\n",
    "    review_dfs = []\n",
    "    for file_name in review_file_names:\n",
    "        file_key = f'{folder_name}/{file_name}'\n",
    "        df = read_s3_csv(bucket_name, file_key)\n",
    "        if 'zigzag' in file_name:\n",
    "            df = process_reviews(df, is_zigzag=True)\n",
    "        review_dfs.append(df)\n",
    "\n",
    "    combined_reviews_df = pd.concat(review_dfs).drop_duplicates().reset_index(drop=True)\n",
    "    print(\"Combined Reviews DataFrame Columns:\")\n",
    "    print(combined_reviews_df.columns)\n",
    "\n",
    "    # ì œí’ˆ íŒŒì¼ í†µí•©\n",
    "    product_dfs = []\n",
    "    platform_mapping = {\n",
    "        '29cm_products.csv': '29cm',\n",
    "        'processed_products.csv': 'musinsa',\n",
    "        'zigzag_products.csv': 'zigzag'\n",
    "    }\n",
    "\n",
    "    for file_name in product_file_names:\n",
    "        file_key = f'{folder_name}/{file_name}'\n",
    "        df = read_s3_csv(bucket_name, file_key)\n",
    "        platform_name = platform_mapping[file_name]\n",
    "        if 'zigzag' in file_name:\n",
    "            df = process_products(df, platform_name, is_zigzag=True)\n",
    "        else:\n",
    "            df = process_products(df, platform_name)\n",
    "        product_dfs.append(df)\n",
    "\n",
    "    combined_products_df = pd.concat(product_dfs).drop_duplicates().reset_index(drop=True)\n",
    "    print(\"Combined Products DataFrame Columns:\")\n",
    "    print(combined_products_df.columns)\n",
    "\n",
    "    # ìƒ‰ìƒ ì¶”ê°€\n",
    "    def add_random_colors(row):\n",
    "        if row['color'] == 'none':\n",
    "            num_colors = random.randint(1, 4)\n",
    "            row['color'] = ', '.join(random.sample(color_list, num_colors))\n",
    "        return row\n",
    "\n",
    "    combined_products_df = combined_products_df.apply(add_random_colors, axis=1)\n",
    "\n",
    "    # CSV íŒŒì¼ ì €ì¥\n",
    "    combined_reviews_df.to_csv('combined_reviews.csv', index=False, encoding='utf-8-sig')\n",
    "    combined_products_df.to_csv('combined_products.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['review_id', 'product_id', 'color', 'size', 'size_comment', 'quality_comment', 'color_comment', 'height', 'weight', 'comment', 'top_size', 'bottom_size', 'product_name']\n",
      "['product_id', 'category', 'description', 'product_name', 'price', 'image_url', 'size', 'color', 'rank']\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# AWS ìê²© ì¦ëª… ì„¤ì •\n",
    "aws_access_key_id = 'PQ'\n",
    "aws_secret_access_key = '6'\n",
    "\n",
    "# S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "\n",
    "bucket_name = 'otto-glue'\n",
    "folder_name = 'integrated-data'\n",
    "\n",
    "combined_reviews_file_key = 'integrated-data/combined_reviews.csv'\n",
    "combined_products_file_key = 'integrated-data/combined_products.csv'\n",
    "test_key = 'non-integrated-data/zigzag_reviews.csv'\n",
    "test_key2 = 'non-integrated-data/zigzag_products.csv'\n",
    "\n",
    "def read_s3_csv(bucket_name, file_key):\n",
    "    obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    df = pd.read_csv(obj['Body'])\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # combined_reviews.csv ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    #combined_reviews_df = read_s3_csv(bucket_name, combined_reviews_file_key)\n",
    "    #print(\"Combined Reviews DataFrame Columns:\")\n",
    "    #print(combined_reviews_df.columns.tolist())\n",
    "\n",
    "    # combined_products.csv ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    #combined_products_df = read_s3_csv(bucket_name, combined_products_file_key)\n",
    "    #print(\"Combined Products DataFrame Columns:\")\n",
    "    #print(combined_products_df.columns.tolist())\n",
    "    \n",
    "    test_df = read_s3_csv(bucket_name, test_key)\n",
    "    test_df2 = read_s3_csv(bucket_name, test_key2)\n",
    "    print(test_df.columns.tolist())\n",
    "    print(test_df2.columns.tolist())\n",
    "    #combined_reviews_df.to_csv('combined_reviews.csv', index=False, encoding='utf-8-sig')\n",
    "    test_df.to_csv('test_zigzag_reviews.csv', index=False, encoding='utf-8-sig')\n",
    "    test_df2.to_csv('test_zigzag_product.csv', index=False, encoding='utf-8-sig' )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬¸ìì—´ ë³€í™˜ :  2024-07-25 14:49:32\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "\n",
    "print(\"ë¬¸ìì—´ ë³€í™˜ : \", now.strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
